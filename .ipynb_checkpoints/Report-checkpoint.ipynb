{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report \n",
    "---\n",
    "This report describe the follow points:\n",
    "    Learning Algorithm,\n",
    "    Plot of Rewards,\n",
    "    Ideas for Future Work.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Learning Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to resolve the problem, we used the DDPA Algorithm, this algorithm uses....\n",
    "\n",
    "\n",
    "Each state and his values that we receive of the environment, we use them to input a neural network as input layer. so, the size of the input layer is the number of the states. the hidden layer processes the information in some layers then return de data to an output layer that is the possible actions to take, so the size of the output layer is de numbers of actions that we can take in this specific environment.\n",
    "\n",
    "The algorithm is deep q-learning with experience replay, first we initialize the memory replay and weights and then we do two process that repeats for train the agent.\n",
    "\n",
    "The first is where we run the environment and observe and collect tuples into a replay memory, for this we have a module to manage it, it can add tuples \"state\", \"action\", \"reward\", \"next state\", \"done\", and store in a fixed memory \n",
    "\n",
    "the second is where and it can sample a few of that memory by a batch-size that we define, so we can select randomly of this memory replay an use (sample method) for train an learn the agent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep Q-Learning Algorithm\n",
    "\n",
    "![title](arq.png)\n",
    "\n",
    "The arquitecture used is the input layer, two hidden layer and a output layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Hyperparameter that we use are:\n",
    "\n",
    "Model: \n",
    "\n",
    "    1 Input Layer,  size = 33\n",
    "    2 Hidden layers   first 2048 nodes and second 1024 nodes  \n",
    "    1 Outpur Layer size = 4\n",
    " \n",
    " \n",
    "Buffer size = 100000\n",
    "\n",
    "Batch Size = 1024      \n",
    "\n",
    "GAMMA = 0.99         (discount factor)\n",
    "\n",
    "TAU = 1e-3           (for soft update of target parameters)\n",
    "\n",
    "LR_ACTOR = 1e-4         # learning rate of the actor \n",
    "\n",
    "LR_CRITIC = 2e-4        # learning rate of the critic\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LR of Actor and Critic values that i used is at this paper \n",
    "https://www.mdpi.com/1424-8220/19/7/1547/pdf Table 2 and Table 3\n",
    "\n",
    "The scenario was solved with 128 batch size and the Agent get 30 points at episode 100, So because at first episodes we get a lot of zero values i tried increment 128 batch size to 1024 and in that order we set the model 2048 - 1048 at the hidden layers in order to have more information to learn quickly.\n",
    "\n",
    "And we get  30 points at episode 68, so that was the final values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Plot of Rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running the training process many times, we get success about 70 episodes, the score we set to succeed is 30.\n",
    "the episodes running is at 300 to verify if the behavior of the Agent is stable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training size 64 hidden layer\n",
    "![title](plot.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Ideas for Future Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The trained model works fine, but sometimes it get stuck, it could be because need more training in order to detect that loops and learn to avoid these, i did some tests with score at 15 in order to get more episodes but the loops appers sometimes, it could be adding more hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

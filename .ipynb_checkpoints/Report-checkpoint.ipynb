{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report \n",
    "---\n",
    "This report describe the follow points:\n",
    "    Learning Algorithm,\n",
    "    Plot of Rewards,\n",
    "    Ideas for Future Work.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Learning Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to resolve the problem, we used the DDPG Algorithm, this algorithm is based in Actor-Critic, \n",
    "\n",
    "We have two models one for the Actor an another for the Critic, Actor-Critic work together, The Actor its inputs are the states and return the actions to execute, and the critic its inputs are states and the actions and return the quality, the QValue, so the Critic evaluate the action that return the actor.\n",
    "\n",
    "And it implements a buffer that store past tuple of state, actions rewards, etc, and it is used as experienced replay. Later we get a batch of that, so we take of this buffer and its is used to learn and we add noise to the return of actions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actor, inputs are states:\n",
    "\n",
    "![title](red.png)\n",
    "\n",
    "Critic, inputs are states and actions\n",
    "![title](red.png)\n",
    "\n",
    "\n",
    "The arquitecture used is the input layer, two hidden layer and a output layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Hyperparameter that we use are:\n",
    "\n",
    "Model for Actor : \n",
    "\n",
    "    1 Input Layer,  size = 33\n",
    "    2 Hidden layers   first 2048 nodes and second 1024 nodes  \n",
    "    1 Outpur Layer size = 4 (action size)\n",
    "\n",
    "Model for Critic : \n",
    "\n",
    "    1 Input Layer,  size = states + actions\n",
    "    2 Hidden layers   first 2048 nodes and second 1024 nodes  \n",
    "    1 Output Layer size = 4 (action size)\n",
    "\n",
    " \n",
    "Buffer size = 100000\n",
    "\n",
    "Batch Size = 1024      \n",
    "\n",
    "GAMMA = 0.99         (discount factor)\n",
    "\n",
    "TAU = 1e-3           (for soft update of target parameters)\n",
    "\n",
    "LR_ACTOR = 1e-4         # learning rate of the actor \n",
    "\n",
    "LR_CRITIC = 2e-4        # learning rate of the critic\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LR of Actor and Critic values that i used is at this paper \n",
    "https://www.mdpi.com/1424-8220/19/7/1547/pdf Table 2 and Table 3\n",
    "\n",
    "The scenario was solved with 128 batch size and the Agent get 30 points at episode 100, So because at first episodes we get a lot of zero values i tried increment 128 batch size to 1024 and in that order we set the model 2048 - 1048 at the hidden layers in order to have more information to learn quickly.\n",
    "\n",
    "And we get  30 points at episode 68, so that was the final values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Plot of Rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running the training process many times, we get success about 70 episodes, the score we set to succeed is 30.\n",
    "the episodes running is at 300 to verify if the behavior of the Agent is stable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![title](plot.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Ideas for Future Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The trained model works fine, i changed hyperparametrs buy the model with 128 size batch apers be more stable. and we can train with PPO Algorithm\n",
    "\n",
    "![title](plot2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
